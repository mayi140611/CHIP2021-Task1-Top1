{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c28001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  7 01:38:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   37C    P0    26W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   77C    P0    70W /  70W |   9208MiB / 15109MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            Off  | 00000000:00:09.0 Off |                    0 |\n",
      "| N/A   38C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d92f6ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at trueto/medbert-base-chinese were not used when initializing Bert: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing Bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Bert were not initialized from the model checkpoint at trueto/medbert-base-chinese and are newly initialized: ['relative_pos_embedding.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  3%|█▏                                     | 99/3111 [03:31<1:53:34,  2.26s/it][99/3111],train loss is:1.012112\n",
      "  4%|█▌                                    | 127/3111 [04:34<1:50:10,  2.22s/it]^C\n",
      "  4%|█▌                                    | 127/3111 [04:35<1:47:52,  2.17s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"./medbert.py\", line 759, in <module>\n",
      "    save_module_path='../checkpoint/medbert2/' + str(fold_) + '.pth'\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ark_nlp/factory/task/base/_sequence_classification.py\", line 398, in fit\n",
      "    loss = self._on_backward(inputs, logits, loss, **kwargs)\n",
      "  File \"./medbert.py\", line 709, in _on_backward\n",
      "    logits = self.module(**inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"./medbert.py\", line 614, in forward\n",
      "    output_hidden_states=True\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1027, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 613, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 498, in forward\n",
      "    past_key_value=self_attn_past_key_value,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 430, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 312, in forward\n",
      "    value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!bash ../predict/predict.sh medbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e558d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ark-nlp==0.0.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep  ark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ba769c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ark_nlp==0.0.2\n",
      "  Downloading ark-nlp-0.0.2.tar.gz (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 461 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ark_nlp==0.0.2) (1.11.0)\n",
      "Requirement already satisfied: tqdm>=4.56.0 in /usr/local/lib/python3.7/dist-packages (from ark_nlp==0.0.2) (4.64.0)\n",
      "Requirement already satisfied: jieba>=0.42.1 in /usr/local/lib/python3.7/dist-packages (from ark_nlp==0.0.2) (0.42.1)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ark_nlp==0.0.2) (4.19.2)\n",
      "Requirement already satisfied: zhon>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from ark_nlp==0.0.2) (1.1.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->ark_nlp==0.0.2) (4.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (2022.4.24)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (0.5.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (3.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (21.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (4.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->ark_nlp==0.0.2) (1.21.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.0->ark_nlp==0.0.2) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.0->ark_nlp==0.0.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.0->ark_nlp==0.0.2) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.0->ark_nlp==0.0.2) (1.26.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.0.0->ark_nlp==0.0.2) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=3.0.0->ark_nlp==0.0.2) (3.7.0)\n",
      "Building wheels for collected packages: ark-nlp\n",
      "  Building wheel for ark-nlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ark-nlp: filename=ark_nlp-0.0.2-py3-none-any.whl size=161142 sha256=08c60b009571006813f19f715ea10424354d8a729296490b5db504441853e8ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/02/2d/0e29613f4fad3a99feac814e0d8957b3595f6401bded2b6a47\n",
      "Successfully built ark-nlp\n",
      "Installing collected packages: ark-nlp\n",
      "  Attempting uninstall: ark-nlp\n",
      "    Found existing installation: ark-nlp 0.0.9\n",
      "    Uninstalling ark-nlp-0.0.9:\n",
      "      Successfully uninstalled ark-nlp-0.0.9\n",
      "Successfully installed ark-nlp-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ark_nlp==0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130b1174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 730/730 [00:00<00:00, 747kB/s]\n",
      "Downloading: 100%|████████████████████████████| 107k/107k [00:00<00:00, 136kB/s]\n",
      "Downloading: 100%|███████████████████████████| 393M/393M [02:08<00:00, 3.20MB/s]\n",
      "Some weights of the model checkpoint at trueto/medbert-base-chinese were not used when initializing Bert: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing Bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Bert were not initialized from the model checkpoint at trueto/medbert-base-chinese and are newly initialized: ['relative_pos_embedding.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|                                                  | 0/1556 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"./medbert.py\", line 759, in <module>\n",
      "    save_module_path='../checkpoint/medbert2/' + str(fold_) + '.pth'\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ark_nlp/factory/task/base/_sequence_classification.py\", line 392, in fit\n",
      "    logits = self.module(**inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"./medbert.py\", line 614, in forward\n",
      "    output_hidden_states=True\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1027, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 613, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 535, in forward\n",
      "    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\", line 241, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 546, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 448, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/activations.py\", line 56, in forward\n",
      "    return self.act(input)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 14.76 GiB total capacity; 10.16 GiB already allocated; 42.75 MiB free; 10.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"./mcbert.py\", line 235, in <module>\n",
      "    bert_vocab = transformers.AutoTokenizer.from_pretrained('../pretrained_model/mc_bert')\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\", line 498, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\", line 368, in get_tokenizer_config\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\", line 685, in get_file_from_repo\n",
      "    use_auth_token=use_auth_token,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\", line 290, in cached_path\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\", line 546, in get_from_cache\n",
      "    \"Connection error, and we cannot find the requested files in the cached path.\"\n",
      "ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.\n",
      "Downloading: 100%|███████████████████████████| 19.0/19.0 [00:00<00:00, 17.7kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 660/660 [00:00<00:00, 721kB/s]\n",
      "Downloading: 100%|████████████████████████████| 107k/107k [00:00<00:00, 136kB/s]\n",
      "Downloading: 100%|████████████████████████████| 263k/263k [00:01<00:00, 252kB/s]\n",
      "Downloading: 100%|███████████████████████████| 2.00/2.00 [00:00<00:00, 1.25kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 112/112 [00:00<00:00, 50.7kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.22G/1.22G [06:03<00:00, 3.59MB/s]\n",
      "Some weights of the model checkpoint at hfl/chinese-macbert-large were not used when initializing Bert: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing Bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Bert were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['relative_pos_embedding.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|                                                  | 0/1556 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"./macbert2-f.py\", line 750, in <module>\n",
      "    save_module_path='../checkpoint/macbert2-f/' + str(fold_) + '.pth'\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ark_nlp/factory/task/base/_sequence_classification.py\", line 392, in fit\n",
      "    logits = self.module(**inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"./macbert2-f.py\", line 607, in forward\n",
      "    output_hidden_states=True\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1027, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 613, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 498, in forward\n",
      "    past_key_value=self_attn_past_key_value,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 430, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 363, in forward\n",
      "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 14.76 GiB total capacity; 10.28 GiB already allocated; 30.75 MiB free; 10.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./dialog_chinese-macbert.py\", line 212, in <module>\r\n",
      "    data_df = get_task_data('../data/source_datasets/train.jsonl')\r\n",
      "  File \"./dialog_chinese-macbert.py\", line 98, in get_task_data\r\n",
      "    with codecs.open(data_path, mode='r', encoding='utf8') as f:\r\n",
      "  File \"/usr/lib/python3.7/codecs.py\", line 904, in open\r\n",
      "    file = builtins.open(filename, mode, buffering)\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/source_datasets/train.jsonl'\r\n"
     ]
    }
   ],
   "source": [
    "!bash train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b709a6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at trueto/medbert-base-chinese were not used when initializing Bert: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing Bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Bert were not initialized from the model checkpoint at trueto/medbert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight', 'relative_pos_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|                                                  | 0/1556 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"./medbert.py\", line 759, in <module>\n",
      "    save_module_path='../checkpoint/medbert2/' + str(fold_) + '.pth'\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ark_nlp/factory/task/base/_sequence_classification.py\", line 392, in fit\n",
      "    logits = self.module(**inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"./medbert.py\", line 614, in forward\n",
      "    output_hidden_states=True\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1027, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 613, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 535, in forward\n",
      "    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\", line 241, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 546, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 448, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/activations.py\", line 56, in forward\n",
      "    return self.act(input)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 14.76 GiB total capacity; 10.16 GiB already allocated; 42.75 MiB free; 10.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python ./medbert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e05b6f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./mcbert.py\", line 235, in <module>\r\n",
      "    bert_vocab = transformers.AutoTokenizer.from_pretrained('../pretrained_model/mc_bert')\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\", line 498, in from_pretrained\r\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\", line 368, in get_tokenizer_config\r\n",
      "    local_files_only=local_files_only,\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\", line 685, in get_file_from_repo\r\n",
      "    use_auth_token=use_auth_token,\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\", line 290, in cached_path\r\n",
      "    local_files_only=local_files_only,\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\", line 546, in get_from_cache\r\n",
      "    \"Connection error, and we cannot find the requested files in the cached path.\"\r\n",
      "ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.\r\n"
     ]
    }
   ],
   "source": [
    "!python ./mcbert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../checkpoint/macbert2-f/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f03f4de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-macbert-large were not used when initializing Bert: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing Bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Bert were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['relative_pos_embedding.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  2%|▌                                      | 99/6221 [05:49<6:05:06,  3.58s/it][99/6221],train loss is:0.990798\n",
      "  3%|█▏                                    | 199/6221 [11:48<6:02:23,  3.61s/it][199/6221],train loss is:0.931933\n",
      "  5%|█▊                                    | 299/6221 [17:46<5:53:31,  3.58s/it][299/6221],train loss is:0.864219\n",
      "  6%|██▍                                   | 399/6221 [23:47<5:52:24,  3.63s/it][399/6221],train loss is:0.861846\n",
      "  8%|███                                   | 499/6221 [29:52<5:50:04,  3.67s/it][499/6221],train loss is:0.836129\n",
      " 10%|███▋                                  | 599/6221 [35:58<5:39:27,  3.62s/it][599/6221],train loss is:0.806574\n",
      " 11%|████▎                                 | 699/6221 [42:02<5:37:22,  3.67s/it][699/6221],train loss is:0.781301\n",
      " 13%|████▉                                 | 799/6221 [48:11<5:37:12,  3.73s/it][799/6221],train loss is:0.764117\n",
      " 14%|█████▍                                | 899/6221 [54:25<5:31:16,  3.73s/it][899/6221],train loss is:0.747559\n",
      " 16%|█████▊                              | 999/6221 [1:00:40<5:26:23,  3.75s/it][999/6221],train loss is:0.731784\n",
      " 18%|██████▏                            | 1099/6221 [1:06:54<5:18:57,  3.74s/it][1099/6221],train loss is:0.720101\n",
      " 19%|██████▋                            | 1199/6221 [1:13:08<5:11:56,  3.73s/it][1199/6221],train loss is:0.711739\n",
      " 21%|███████▎                           | 1299/6221 [1:19:22<5:05:53,  3.73s/it][1299/6221],train loss is:0.702044\n",
      " 22%|███████▊                           | 1399/6221 [1:25:36<4:59:58,  3.73s/it][1399/6221],train loss is:0.693082\n",
      " 24%|████████▍                          | 1499/6221 [1:31:50<4:53:46,  3.73s/it][1499/6221],train loss is:0.685201\n",
      " 26%|████████▉                          | 1599/6221 [1:38:05<4:47:23,  3.73s/it][1599/6221],train loss is:0.679839\n",
      " 27%|█████████▌                         | 1699/6221 [1:44:19<4:41:36,  3.74s/it][1699/6221],train loss is:0.674349\n",
      " 29%|██████████                         | 1799/6221 [1:50:34<4:35:55,  3.74s/it][1799/6221],train loss is:0.668863\n",
      " 31%|██████████▋                        | 1899/6221 [1:56:49<4:29:14,  3.74s/it][1899/6221],train loss is:0.664981\n",
      " 32%|███████████▏                       | 1999/6221 [2:03:03<4:24:01,  3.75s/it][1999/6221],train loss is:0.660255\n",
      " 34%|███████████▊                       | 2099/6221 [2:09:10<4:01:02,  3.51s/it][2099/6221],train loss is:0.656017\n",
      " 35%|████████████▎                      | 2199/6221 [2:15:04<4:00:21,  3.59s/it][2199/6221],train loss is:0.651781\n",
      " 37%|████████████▉                      | 2299/6221 [2:21:06<3:57:56,  3.64s/it][2299/6221],train loss is:0.648624\n",
      " 39%|█████████████▍                     | 2399/6221 [2:27:12<3:53:57,  3.67s/it][2399/6221],train loss is:0.645757\n",
      " 40%|██████████████                     | 2499/6221 [2:33:24<3:52:05,  3.74s/it][2499/6221],train loss is:0.642355\n",
      " 42%|██████████████▌                    | 2599/6221 [2:39:38<3:46:18,  3.75s/it][2599/6221],train loss is:0.639689\n",
      " 43%|███████████████▏                   | 2699/6221 [2:45:53<3:39:37,  3.74s/it][2699/6221],train loss is:0.637068\n",
      " 45%|███████████████▋                   | 2799/6221 [2:52:08<3:34:49,  3.77s/it][2799/6221],train loss is:0.634881\n",
      " 47%|████████████████▎                  | 2899/6221 [2:58:24<3:27:55,  3.76s/it][2899/6221],train loss is:0.632120\n",
      " 48%|████████████████▊                  | 2999/6221 [3:04:39<3:21:18,  3.75s/it][2999/6221],train loss is:0.630046\n",
      " 50%|█████████████████▍                 | 3099/6221 [3:10:54<3:15:26,  3.76s/it][3099/6221],train loss is:0.627868\n",
      " 51%|█████████████████▉                 | 3199/6221 [3:17:09<3:08:43,  3.75s/it][3199/6221],train loss is:0.625418\n",
      " 53%|██████████████████▌                | 3299/6221 [3:23:23<3:01:59,  3.74s/it][3299/6221],train loss is:0.624170\n",
      " 55%|███████████████████                | 3399/6221 [3:29:38<2:56:31,  3.75s/it][3399/6221],train loss is:0.622051\n",
      " 56%|███████████████████▋               | 3499/6221 [3:35:53<2:49:48,  3.74s/it][3499/6221],train loss is:0.620399\n",
      " 58%|████████████████████▏              | 3599/6221 [3:41:56<2:37:02,  3.59s/it][3599/6221],train loss is:0.618699\n",
      " 59%|████████████████████▊              | 3699/6221 [3:47:57<2:32:36,  3.63s/it][3699/6221],train loss is:0.616416\n",
      " 61%|█████████████████████▎             | 3799/6221 [3:54:03<2:28:06,  3.67s/it][3799/6221],train loss is:0.614940\n",
      " 63%|█████████████████████▉             | 3899/6221 [4:00:12<2:23:19,  3.70s/it][3899/6221],train loss is:0.612816\n",
      " 64%|██████████████████████▍            | 3999/6221 [4:06:24<2:18:22,  3.74s/it][3999/6221],train loss is:0.611148\n",
      " 66%|███████████████████████            | 4099/6221 [4:12:39<2:13:04,  3.76s/it][4099/6221],train loss is:0.609757\n",
      " 67%|███████████████████████▌           | 4199/6221 [4:18:54<2:06:37,  3.76s/it][4199/6221],train loss is:0.608158\n",
      " 69%|████████████████████████▏          | 4299/6221 [4:25:10<2:00:07,  3.75s/it][4299/6221],train loss is:0.606484\n",
      " 71%|████████████████████████▋          | 4399/6221 [4:31:25<1:54:04,  3.76s/it][4399/6221],train loss is:0.604786\n",
      " 72%|█████████████████████████▎         | 4499/6221 [4:37:40<1:47:43,  3.75s/it][4499/6221],train loss is:0.604550\n",
      " 74%|█████████████████████████▊         | 4599/6221 [4:43:55<1:41:20,  3.75s/it][4599/6221],train loss is:0.603103\n",
      " 76%|██████████████████████████▍        | 4699/6221 [4:50:10<1:35:30,  3.77s/it][4699/6221],train loss is:0.601907\n",
      " 77%|██████████████████████████▉        | 4799/6221 [4:56:25<1:28:44,  3.74s/it][4799/6221],train loss is:0.600591\n",
      " 79%|███████████████████████████▌       | 4899/6221 [5:02:39<1:22:25,  3.74s/it][4899/6221],train loss is:0.599234\n",
      " 80%|████████████████████████████       | 4999/6221 [5:08:54<1:16:21,  3.75s/it][4999/6221],train loss is:0.598545\n",
      " 82%|████████████████████████████▋      | 5099/6221 [5:15:09<1:09:55,  3.74s/it][5099/6221],train loss is:0.597746\n",
      " 84%|█████████████████████████████▎     | 5199/6221 [5:21:24<1:03:35,  3.73s/it][5199/6221],train loss is:0.596807\n",
      " 85%|███████████████████████████████▌     | 5299/6221 [5:27:38<57:37,  3.75s/it][5299/6221],train loss is:0.595594\n",
      " 87%|████████████████████████████████     | 5399/6221 [5:33:53<51:11,  3.74s/it][5399/6221],train loss is:0.594128\n",
      " 88%|████████████████████████████████▋    | 5499/6221 [5:40:07<45:00,  3.74s/it][5499/6221],train loss is:0.593018\n",
      " 90%|█████████████████████████████████▎   | 5599/6221 [5:46:22<38:53,  3.75s/it][5599/6221],train loss is:0.591705\n",
      " 92%|█████████████████████████████████▉   | 5699/6221 [5:52:37<32:43,  3.76s/it][5699/6221],train loss is:0.590880\n",
      " 93%|██████████████████████████████████▍  | 5799/6221 [5:58:52<26:19,  3.74s/it][5799/6221],train loss is:0.590462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████  | 5899/6221 [6:05:06<20:08,  3.75s/it][5899/6221],train loss is:0.590453\n",
      " 96%|███████████████████████████████████▋ | 5999/6221 [6:11:21<13:50,  3.74s/it][5999/6221],train loss is:0.589917\n",
      " 98%|████████████████████████████████████▎| 6099/6221 [6:17:36<07:37,  3.75s/it][6099/6221],train loss is:0.589331\n",
      "100%|████████████████████████████████████▊| 6199/6221 [6:23:51<01:22,  3.76s/it][6199/6221],train loss is:0.588463\n",
      "100%|█████████████████████████████████████| 6221/6221 [6:25:12<00:00,  3.72s/it]\n",
      "epoch:[0],train loss is:0.588332 \n",
      "\n",
      "classification_report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         不标注       0.89      0.84      0.86      2102\n",
      "          其他       0.79      0.75      0.77       467\n",
      "          阳性       0.94      0.96      0.95      7219\n",
      "          阴性       0.92      0.89      0.91      1271\n",
      "\n",
      "    accuracy                           0.92     11059\n",
      "   macro avg       0.89      0.86      0.87     11059\n",
      "weighted avg       0.92      0.92      0.92     11059\n",
      "\n",
      "confusion_matrix_: \n",
      " [[1764   26  281   31]\n",
      " [  17  352   72   26]\n",
      " [ 173   37 6966   43]\n",
      " [  33   29   75 1134]]\n",
      "test loss is:0.484166,test acc is:0.923772,f1_score is:0.873592\n",
      "  0%|                                                  | 0/6221 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"./macbert2-f.py\", line 750, in <module>\n",
      "    save_module_path='../checkpoint/macbert2-f/' + str(fold_) + '.pth'\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ark_nlp/factory/task/base/_sequence_classification.py\", line 392, in fit\n",
      "    logits = self.module(**inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"./macbert2-f.py\", line 607, in forward\n",
      "    output_hidden_states=True\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1027, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 613, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 535, in forward\n",
      "    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\", line 241, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 546, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 448, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/activations.py\", line 56, in forward\n",
      "    return self.act(input)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 14.76 GiB total capacity; 13.28 GiB already allocated; 25.75 MiB free; 13.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python ./macbert2-f.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34d2bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./dialog_chinese-macbert.py\", line 212, in <module>\r\n",
      "    data_df = get_task_data('../data/source_datasets/train.jsonl')\r\n",
      "  File \"./dialog_chinese-macbert.py\", line 98, in get_task_data\r\n",
      "    with codecs.open(data_path, mode='r', encoding='utf8') as f:\r\n",
      "  File \"/usr/lib/python3.7/codecs.py\", line 904, in open\r\n",
      "    file = builtins.open(filename, mode, buffering)\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/source_datasets/train.jsonl'\r\n"
     ]
    }
   ],
   "source": [
    "!python ./dialog_chinese-macbert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a746c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
